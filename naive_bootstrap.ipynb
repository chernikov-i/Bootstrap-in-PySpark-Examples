{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011ddcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window, functions as F, types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae74820a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/23 12:11:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "os.environ['PYSPARK_PYTHON']= sys.executable\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[1]\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7197601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------+------+----------+--------------------+\n",
      "|camp_id|control_group_flg|revenue|margin|conversion|             user_id|\n",
      "+-------+-----------------+-------+------+----------+--------------------+\n",
      "|ED_3755|                0|   4.77|   5.3|         1|6f9be979ec9f4aba9...|\n",
      "|ED_3755|                0|   7.53|  8.12|         1|6a0a4bdd10e670717...|\n",
      "|ED_3755|                0|  12.33|  5.24|         1|2b96123b22c810b53...|\n",
      "|ED_3755|                0|   5.44|  4.32|         1|06d37e5b5d6c45a9a...|\n",
      "|ED_3755|                0|    0.0|   0.0|         0|86fcb416dd0d2b9e7...|\n",
      "+-------+-----------------+-------+------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField('camp_id', T.StringType()),\n",
    "    T.StructField('control_group_flg', T.ShortType()),\n",
    "    T.StructField('revenue', T.DoubleType()),\n",
    "    T.StructField('margin', T.DoubleType()),\n",
    "    T.StructField('conversion', T.ShortType()),\n",
    "    T.StructField('user_id', T.StringType()),\n",
    "])\n",
    "\n",
    "statistics_df = spark.createDataFrame(pd.read_csv('data_sample.csv'))\n",
    "statistics_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ea0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список ключевых столбцов групп\n",
    "KEY_COLS = ['camp_id', 'control_group_flg']\n",
    "# Список метрик, по которым считаем статистику\n",
    "STATISTICS_COLS = ['revenue', 'margin', 'conversion']\n",
    "# Количество бутстрап операций\n",
    "BS_ITERS = 10000\n",
    "ALPHA = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25c8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рассчитываем количество строк в группе и порядковый номер\n",
    "window_for_bs_stats = Window.partitionBy(*KEY_COLS)\n",
    "statistics_df = (\n",
    "    statistics_df\n",
    "    .withColumn('rn',\n",
    "                F.row_number().over(window_for_bs_stats.orderBy('user_id')))\n",
    "    .withColumn('group_cnt', F.count(F.lit(1)).over(window_for_bs_stats))\n",
    "    # repartition нужен для того, чтобы дальнейшие действия по разворачиванию происходили на всех экзекьюторах\n",
    "    # а не ограниченном количестве, соответствующем количеству партиций из window_for_bs_stats\n",
    "    .repartition('rn')\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0759962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_df = (\n",
    "    statistics_df\n",
    "    # Для каждой записи в нашем датафрейме генерируем количество строк = bs_iter_cnt\n",
    "    # Порядковый номер строки будет являться номером итерации\n",
    "    .select(KEY_COLS + ['group_cnt'])\n",
    "    .withColumn('iter_num', F.explode(F.sequence(F.lit(1), F.lit(BS_ITERS))))\n",
    "    # Для каждой строки в итерации случайно выбираем номер записи из изначального датафрейма\n",
    "    .withColumn('rn', \n",
    "                (F.floor(F.rand() * (F.col('group_cnt'))) + 1).cast('int')\n",
    "                .alias('rn'))\n",
    "    .select(KEY_COLS + ['iter_num', 'rn'])\n",
    ")\n",
    "\n",
    "# Соединяем бутстрап датафрейм с изначальным по случайному идентификатору строки\n",
    "result_df = (\n",
    "    bs_df\n",
    "    .join(\n",
    "        statistics_df.select(KEY_COLS + STATISTICS_COLS + ['rn']),\n",
    "        on=KEY_COLS + ['rn'],\n",
    "        how='inner'\n",
    "    )\n",
    "    # Группируем по ключевым полям и номеру итерации\n",
    "    .groupby(KEY_COLS + ['iter_num'])\n",
    "    .agg(*[F.avg(stat_col).alias(stat_col) for stat_col in STATISTICS_COLS])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4f2bc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>camp_id</th>\n",
       "      <th>revenue</th>\n",
       "      <th>margin</th>\n",
       "      <th>conversion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ED_3755</td>\n",
       "      <td>[0.16221133363306647, 1.2243871651754177]</td>\n",
       "      <td>[-0.8159217680834554, 0.2912118966302792]</td>\n",
       "      <td>[0.011798244376076017, 0.0679830485921541]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   camp_id                                    revenue  \\\n",
       "0  ED_3755  [0.16221133363306647, 1.2243871651754177]   \n",
       "\n",
       "                                      margin  \\\n",
       "0  [-0.8159217680834554, 0.2912118966302792]   \n",
       "\n",
       "                                   conversion  \n",
       "0  [0.011798244376076017, 0.0679830485921541]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сохраняем результат и приводим к единому виду с остальными тетрадками\n",
    "result = result_df.toPandas()\n",
    "# Минусуем метрики контрольной группы, чтобы сложить можно было для получения разницы\n",
    "result.loc[result['control_group_flg'] == 1, STATISTICS_COLS] *= -1\n",
    "# Можно собирать и сразу в pyspark\n",
    "result = (\n",
    "    result\n",
    "    .groupby(['camp_id', 'iter_num'], as_index=False).sum()\n",
    "    .groupby('camp_id', as_index=False)[STATISTICS_COLS].quantile([ALPHA/2, 1-ALPHA/2])\n",
    "    .groupby('camp_id', as_index=False).agg(list)\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91015e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b2c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
